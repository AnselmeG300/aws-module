# LAB 08 â€” Politiques S3 et Transferts de Fichiers entre Buckets

## ğŸ¯ Objectifs

Ã€ la fin de ce lab, vous serez capable de :
- âœ… Comprendre les **politiques de bucket S3** (Bucket Policies)
- âœ… Configurer les **politiques IAM** pour accÃ©der Ã  S3
- âœ… **Copier des fichiers** d'un bucket S3 Ã  un autre
- âœ… Mettre en place la **rÃ©plication S3** (mÃªme rÃ©gion et cross-region)
- âœ… GÃ©rer les **permissions cross-account** (accÃ¨s entre comptes AWS)
- âœ… Automatiser les transferts avec **AWS CLI** et **scripts**
- âœ… Configurer les **lifecycle policies** pour gÃ©rer le cycle de vie des objets

---

## ğŸ“‹ PrÃ©requis

- âœ… AccÃ¨s Ã  la console AWS
- âœ… RÃ©gion : **N. Virginia (us-east-1)** et **Ohio (us-east-2)** pour cross-region
- âœ… AccÃ¨s Ã  **CloudShell** (AWS CLI dÃ©jÃ  configurÃ©)
- âœ… Permissions IAM suffisantes (S3FullAccess pour ce lab)
- âœ… Navigateur web (pour la console + CloudShell)

---

## ğŸ“š Concepts clÃ©s

### ğŸ” Types de politiques S3

| Type | Niveau | Cas d'usage |
|------|--------|-------------|
| **Bucket Policy** | Bucket | ContrÃ´ler l'accÃ¨s au bucket entier ou Ã  des prÃ©fixes |
| **IAM Policy** | Utilisateur/RÃ´le | DÃ©finir ce qu'un utilisateur peut faire sur S3 |
| **ACL (Access Control List)** | Objet/Bucket | Legacy, Ã©viter si possible |
| **S3 Block Public Access** | Compte/Bucket | Bloquer tout accÃ¨s public |

### ğŸ”„ MÃ©thodes de copie entre buckets

| MÃ©thode | Cas d'usage | Temps rÃ©el | Historique |
|---------|-------------|------------|------------|
| **AWS CLI (`aws s3 cp`)** | Copie manuelle/scriptÃ©e | Non | Non |
| **S3 Batch Operations** | Copie massive (millions d'objets) | Non | Oui |
| **S3 Replication** | Copie automatique continue | Oui | Optionnel |
| **DataSync** | Migration massive avec validation | Non | Non |
| **SDK (Boto3, etc.)** | Automatisation custom | Selon code | Non |

---

## ğŸ—ï¸ Architecture du lab

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Compte AWS                           â”‚
â”‚                                                             â”‚
â”‚  Region: us-east-1 (N. Virginia)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚  Bucket Source           â”‚                               â”‚
â”‚  â”‚  m2i-source-[nom]-bucket â”‚                               â”‚
â”‚  â”‚                          â”‚                               â”‚
â”‚  â”‚  ğŸ“„ fichier1.txt         â”‚                               â”‚
â”‚  â”‚  ğŸ“„ fichier2.json        â”‚                               â”‚
â”‚  â”‚  ğŸ“ dossier/             â”‚                               â”‚
â”‚  â”‚     ğŸ“„ fichier3.csv      â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚             â”‚                                                â”‚
â”‚             â”‚ Copie manuelle (AWS CLI)                       â”‚
â”‚             â”‚ ou RÃ©plication automatique                     â”‚
â”‚             â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚  Bucket Destination 1    â”‚                               â”‚
â”‚  â”‚  m2i-dest1-[nom]-bucket  â”‚ (mÃªme rÃ©gion)                 â”‚
â”‚  â”‚                          â”‚                               â”‚
â”‚  â”‚  ğŸ“„ fichier1.txt (copie) â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                             â”‚
â”‚  Region: us-east-2 (Ohio)                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚  Bucket Destination 2    â”‚                               â”‚
â”‚  â”‚  m2i-dest2-[nom]-bucket  â”‚ (cross-region)                â”‚
â”‚  â”‚                          â”‚                               â”‚
â”‚  â”‚  ğŸ“„ fichier1.txt (copie) â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ PARTIE 1 : CrÃ©er les buckets et uploader des fichiers

### Ã‰tape 1.1 : CrÃ©er les buckets S3

#### Via la console AWS

1. **Console AWS â†’ S3 â†’ Create bucket**

2. **Bucket source (us-east-1)** :
   - **Bucket name** : `m2i-source-[votre-prenom]-bucket`
   - Exemple : `m2i-source-anselme-bucket`
   - **Region** : `US East (N. Virginia) us-east-1`
   - **Block Public Access** : âœ… Laissez tout cochÃ© (sÃ©curitÃ©)
   - **Versioning** : âœ… Enable (important pour la rÃ©plication)
   - **Default encryption** : Server-side encryption with Amazon S3 managed keys (SSE-S3)
   - **Cliquez sur "Create bucket"**

3. **Bucket destination 1 (mÃªme rÃ©gion)** :
   - **Bucket name** : `m2i-dest1-[votre-prenom]-bucket`
   - **Region** : `US East (N. Virginia) us-east-1`
   - **Versioning** : âœ… Enable
   - **Create bucket**

4. **Bucket destination 2 (cross-region)** :
   - **Bucket name** : `m2i-dest2-[votre-prenom]-bucket`
   - **Region** : `US East (Ohio) us-east-2` â† DiffÃ©rent !
   - **Versioning** : âœ… Enable
   - **Create bucket**

#### Via CloudShell (AWS CLI)

```bash
# Variables
prenom="anselme"  # â† Changez avec votre prenom
source_bucket="m2i-source-$prenom-bucket"
dest1_bucket="m2i-dest1-$prenom-bucket"
dest2_bucket="m2i-dest2-$prenom-bucket"

# Creer le bucket source (us-east-1)
aws s3 mb "s3://$source_bucket" --region us-east-1

# Activer le versioning
aws s3api put-bucket-versioning \
  --bucket "$source_bucket" \
  --versioning-configuration Status=Enabled

# Creer bucket destination 1 (meme region)
aws s3 mb "s3://$dest1_bucket" --region us-east-1
aws s3api put-bucket-versioning \
  --bucket "$dest1_bucket" \
  --versioning-configuration Status=Enabled

# Creer bucket destination 2 (cross-region)
aws s3 mb "s3://$dest2_bucket" --region us-east-2
aws s3api put-bucket-versioning \
  --bucket "$dest2_bucket" \
  --versioning-configuration Status=Enabled

echo "Buckets crees avec succes."
```

---

### Ã‰tape 1.2 : CrÃ©er des fichiers de test

```bash
# Creer un dossier de travail dans CloudShell
mkdir -p ~/s3-lab/dossier
cd ~/s3-lab

# Creer fichier1.txt
cat <<EOF > fichier1.txt
Ceci est un fichier de test pour le LAB S3.
Date de creation: $(date)
Auteur: M2i Formation
EOF

# Creer fichier2.json
cat <<EOF > fichier2.json
{
  "lab": "LAB-08-S3",
  "objectif": "Politiques et transferts S3",
  "date": "$(date +%F)",
  "region": "us-east-1"
}
EOF

# Creer un sous-dossier avec fichier3.csv
cat <<EOF > dossier/fichier3.csv
Nom,Prenom,Age,Ville
Dupont,Jean,30,Paris
Martin,Marie,25,Lyon
Bernard,Luc,35,Marseille
EOF

# Creer fichier4.log (gros fichier)
for i in $(seq 1 1000); do
  echo "[$i] Log entry at $(date +%H:%M:%S) - Random data: $(uuidgen)"
done > fichier4.log

echo "Fichiers de test crees dans ~/s3-lab"
ls -R
```

---

### Ã‰tape 1.3 : Uploader les fichiers dans le bucket source

```bash
# Variables
source_bucket="m2i-source-anselme-bucket"  # â† Votre bucket

# Uploader tous les fichiers
aws s3 sync . "s3://$source_bucket/"

# Verifier l'upload
echo "\nContenu du bucket source :"
aws s3 ls "s3://$source_bucket/" --recursive --human-readable

# Resultat attendu :
# 2026-02-05 10:30:00   150 Bytes fichier1.txt
# 2026-02-05 10:30:00   180 Bytes fichier2.json
# 2026-02-05 10:30:00   120 Bytes dossier/fichier3.csv
# 2026-02-05 10:30:00    50 KB fichier4.log
```

---

## ğŸ” PARTIE 2 : Configurer les politiques de bucket

### Ã‰tape 2.1 : CrÃ©er une politique de bucket (Bucket Policy)

#### Politique 1 : Permettre la lecture depuis le bucket destination

**ScÃ©nario** : Autoriser le bucket destination Ã  lire les objets du bucket source.

```bash
# Creer le fichier de politique
source_bucket="m2i-source-anselme-bucket"  # â† Votre bucket

cat <<EOF > bucket-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowDestinationBucketRead",
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": [
        "s3:GetObject",
        "s3:GetObjectVersion"
      ],
      "Resource": "arn:aws:s3:::$source_bucket/*",
      "Condition": {
        "StringEquals": {
          "s3:x-amz-server-side-encryption": "AES256"
        }
      }
    },
    {
      "Sid": "AllowListBucket",
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": "s3:ListBucket",
      "Resource": "arn:aws:s3:::$source_bucket"
    }
  ]
}
EOF

# Appliquer la politique au bucket source
aws s3api put-bucket-policy \
  --bucket "$source_bucket" \
  --policy file://bucket-policy.json

echo "Politique de bucket appliquee."
```

**ğŸ’¡ Explication de la politique** :

| Ã‰lÃ©ment | Valeur | Explication |
|---------|--------|-------------|
| **Principal** | `"Service": "s3.amazonaws.com"` | Service S3 autorisÃ© |
| **Action** | `s3:GetObject`, `s3:ListBucket` | Lire les objets et lister le bucket |
| **Resource** | `arn:aws:s3:::bucket/*` | Tous les objets du bucket |
| **Condition** | Chiffrement AES256 | Seulement si objets chiffrÃ©s |

---

#### Politique 2 : Permettre l'Ã©criture dans le bucket destination

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowReplicationWrite",
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": [
        "s3:ReplicateObject",
        "s3:ReplicateDelete",
        "s3:ReplicateTags"
      ],
      "Resource": "arn:aws:s3:::m2i-dest1-anselme-bucket/*"
    }
  ]
}
```

```bash
# Appliquer au bucket destination
aws s3api put-bucket-policy \
  --bucket m2i-dest1-anselme-bucket \
  --policy file://dest-bucket-policy.json
```

---

### Ã‰tape 2.2 : CrÃ©er une politique IAM pour l'utilisateur

**ScÃ©nario** : Permettre Ã  votre utilisateur IAM de copier des fichiers entre buckets.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowS3ListAllBuckets",
      "Effect": "Allow",
      "Action": "s3:ListAllMyBuckets",
      "Resource": "*"
    },
    {
      "Sid": "AllowReadSourceBucket",
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:GetObjectVersion",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::m2i-source-anselme-bucket",
        "arn:aws:s3:::m2i-source-anselme-bucket/*"
      ]
    },
    {
      "Sid": "AllowWriteDestBuckets",
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:PutObjectAcl",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::m2i-dest1-anselme-bucket",
        "arn:aws:s3:::m2i-dest1-anselme-bucket/*",
        "arn:aws:s3:::m2i-dest2-anselme-bucket",
        "arn:aws:s3:::m2i-dest2-anselme-bucket/*"
      ]
    }
  ]
}
```

**Appliquer la politique** :

```bash
# Via console : IAM â†’ Policies â†’ Create policy â†’ JSON â†’ Copier/coller
# Nom : S3-Copy-Between-Buckets-Policy

# Ou via CLI :
aws iam create-policy \
  --policy-name S3-Copy-Between-Buckets-Policy \
  --policy-document file://iam-policy.json

# Attacher Ã  votre utilisateur
aws iam attach-user-policy \
  --user-name benoit \
  --policy-arn arn:aws:iam::123456789012:policy/S3-Copy-Between-Buckets-Policy
```

---

## ğŸ“‹ PARTIE 3 : Copier des fichiers entre buckets

### MÃ©thode 1 : Copie manuelle avec AWS CLI

#### Copier un fichier unique

```bash
# Copier fichier1.txt du source vers destination 1
aws s3 cp \
  s3://m2i-source-anselme-bucket/fichier1.txt \
  s3://m2i-dest1-anselme-bucket/fichier1.txt

# VÃ©rifier la copie
aws s3 ls s3://m2i-dest1-anselme-bucket/

# âœ… RÃ©sultat : 2026-02-05 10:35:00   150 Bytes fichier1.txt
```

#### Copier un dossier entier

```bash
# Copier le dossier "dossier/" avec tous ses fichiers
aws s3 cp \
  s3://m2i-source-anselme-bucket/dossier/ \
  s3://m2i-dest1-anselme-bucket/dossier/ \
  --recursive

# VÃ©rifier
aws s3 ls s3://m2i-dest1-anselme-bucket/dossier/ --recursive
```

#### Synchroniser deux buckets (sync)

```bash
# Synchroniser tous les fichiers (copie uniquement ce qui a changÃ©)
aws s3 sync \
  s3://m2i-source-anselme-bucket/ \
  s3://m2i-dest1-anselme-bucket/

# Avec filtres
aws s3 sync \
  s3://m2i-source-anselme-bucket/ \
  s3://m2i-dest1-anselme-bucket/ \
  --exclude "*" \
  --include "*.txt" \
  --include "*.json"

# âœ… Ne copie que les fichiers .txt et .json
```

#### Copie cross-region

```bash
# Copier vers bucket dans une autre rÃ©gion (us-east-2)
aws s3 sync \
  s3://m2i-source-anselme-bucket/ \
  s3://m2i-dest2-anselme-bucket/ \
  --region us-east-2

echo "Copie cross-region terminee."
```

---

### Methode 2 : Script bash automatise (CloudShell)

```bash
# Creer un script de copie avec rapport
cat <<'EOF' > copy_s3_buckets.sh
#!/usr/bin/env bash
set -euo pipefail

SOURCE_BUCKET="${1:-m2i-source-anselme-bucket}"
DEST_BUCKET="${2:-m2i-dest1-anselme-bucket}"
PREFIX="${3:-}"
DRY_RUN="${4:-false}"

echo "Demarrage de la copie S3"
echo "Source: s3://$SOURCE_BUCKET/$PREFIX"
echo "Destination: s3://$DEST_BUCKET/$PREFIX"

objects_json=$(aws s3api list-objects-v2 --bucket "$SOURCE_BUCKET" --prefix "$PREFIX" --output json)
total_objects=$(echo "$objects_json" | python -c 'import json,sys;print(len(json.load(sys.stdin).get("Contents",[])))')

if [ "$total_objects" -eq 0 ]; then
  echo "Aucun objet trouve dans le bucket source"
  exit 1
fi

copied=0
errors=0

echo "$objects_json" | python - <<'PY'
import json,sys
data=json.load(sys.stdin)
for obj in data.get("Contents",[]):
    print(obj["Key"])
PY
| while read -r key; do
  if [ "$DRY_RUN" = "true" ]; then
    echo "[DRY RUN] Copie: $key"
  else
    if aws s3 cp "s3://$SOURCE_BUCKET/$key" "s3://$DEST_BUCKET/$key" --quiet; then
      copied=$((copied+1))
      echo "OK: $key"
    else
      errors=$((errors+1))
      echo "Erreur copie: $key"
    fi
  fi
done

echo "Rapport: copies=$copied erreurs=$errors"

dest_count=$(aws s3api list-objects-v2 --bucket "$DEST_BUCKET" --prefix "$PREFIX" --query 'length(Contents)' --output text)
if [ "$dest_count" = "$total_objects" ]; then
  echo "Verification OK: $dest_count/$total_objects"
else
  echo "Attention: $dest_count/$total_objects"
fi
EOF

chmod +x copy_s3_buckets.sh
```

**Utilisation** :

```bash
# Copie normale
./copy_s3_buckets.sh "m2i-source-anselme-bucket" "m2i-dest1-anselme-bucket"

# Copie avec prefixe (seulement le dossier "dossier/")
./copy_s3_buckets.sh "m2i-source-anselme-bucket" "m2i-dest1-anselme-bucket" "dossier/"

# Mode dry-run (simulation)
./copy_s3_buckets.sh "m2i-source-anselme-bucket" "m2i-dest1-anselme-bucket" "" true
```

---

## ğŸ”„ PARTIE 4 : Configurer la rÃ©plication S3 automatique

### Ã‰tape 4.1 : CrÃ©er un rÃ´le IAM pour la rÃ©plication

```bash
# Politique de confiance (Trust Policy)
cat <<'EOF' > trust-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

# Creer le role
aws iam create-role \
  --role-name S3-Replication-Role \
  --assume-role-policy-document file://trust-policy.json

# Politique de permissions
cat <<EOF > replication-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetReplicationConfiguration",
        "s3:ListBucket"
      ],
      "Resource": "arn:aws:s3:::m2i-source-anselme-bucket"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObjectVersionForReplication",
        "s3:GetObjectVersionAcl",
        "s3:GetObjectVersionTagging"
      ],
      "Resource": "arn:aws:s3:::m2i-source-anselme-bucket/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:ReplicateObject",
        "s3:ReplicateDelete",
        "s3:ReplicateTags"
      ],
      "Resource": [
        "arn:aws:s3:::m2i-dest1-anselme-bucket/*",
        "arn:aws:s3:::m2i-dest2-anselme-bucket/*"
      ]
    }
  ]
}
EOF

# Creer et attacher la politique
aws iam put-role-policy \
  --role-name S3-Replication-Role \
  --policy-name S3-Replication-Policy \
  --policy-document file://replication-policy.json

echo "Role IAM cree : S3-Replication-Role"
```

---

### Ã‰tape 4.2 : Configurer la rÃ©plication

#### Configuration de rÃ©plication (Same-Region Replication - SRR)

```bash
# Configuration de replication
account_id=$(aws sts get-caller-identity --query Account --output text)

cat <<EOF > replication-config.json
{
  "Role": "arn:aws:iam::${account_id}:role/S3-Replication-Role",
  "Rules": [
    {
      "ID": "ReplicateAll",
      "Status": "Enabled",
      "Priority": 1,
      "DeleteMarkerReplication": {
        "Status": "Enabled"
      },
      "Filter": {},
      "Destination": {
        "Bucket": "arn:aws:s3:::m2i-dest1-anselme-bucket",
        "ReplicationTime": {
          "Status": "Enabled",
          "Time": {
            "Minutes": 15
          }
        },
        "Metrics": {
          "Status": "Enabled",
          "EventThreshold": {
            "Minutes": 15
          }
        }
      }
    }
  ]
}
EOF

# Appliquer la configuration
aws s3api put-bucket-replication \
  --bucket m2i-source-anselme-bucket \
  --replication-configuration file://replication-config.json

echo "Replication configuree."
```

#### Configuration Cross-Region Replication (CRR)

```json
{
  "Role": "arn:aws:iam::123456789012:role/S3-Replication-Role",
  "Rules": [
    {
      "ID": "ReplicateCrossRegion",
      "Status": "Enabled",
      "Priority": 2,
      "Filter": {
        "Prefix": "cross-region/"
      },
      "Destination": {
        "Bucket": "arn:aws:s3:::m2i-dest2-anselme-bucket",
        "StorageClass": "STANDARD_IA"
      }
    }
  ]
}
```

```bash
# Appliquer CRR
aws s3api put-bucket-replication \
  --bucket m2i-source-anselme-bucket \
  --replication-configuration file://crr-config.json
```

---

### Ã‰tape 4.3 : Tester la rÃ©plication

```bash
# Uploader un nouveau fichier dans le bucket source
echo "Test replication" > test-replication.txt

aws s3 cp test-replication.txt s3://m2i-source-anselme-bucket/

# Attendre quelques secondes
sleep 10

# Verifier dans le bucket destination
if aws s3 ls s3://m2i-dest1-anselme-bucket/test-replication.txt >/dev/null 2>&1; then
  echo "Replication reussie."
else
  echo "Replication en cours (peut prendre jusqu'a 15 min)."
fi

# Verifier le statut de replication
aws s3api head-object \
  --bucket m2i-dest1-anselme-bucket \
  --key test-replication.txt \
  --query 'ReplicationStatus'
```

---

## ğŸ“¦ PARTIE 5 : Gestion du cycle de vie (Lifecycle Policies)

### Ã‰tape 5.1 : CrÃ©er une politique de cycle de vie

**ScÃ©nario** : 
- Fichiers non accÃ©dÃ©s depuis 30 jours â†’ Transition vers STANDARD_IA
- Fichiers non accÃ©dÃ©s depuis 90 jours â†’ Transition vers GLACIER
- Supprimer aprÃ¨s 365 jours

```json
{
  "Rules": [
    {
      "Id": "ArchiveOldFiles",
      "Status": "Enabled",
      "Filter": {},
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 90,
          "StorageClass": "GLACIER"
        }
      ],
      "Expiration": {
        "Days": 365
      },
      "NoncurrentVersionTransitions": [
        {
          "NoncurrentDays": 30,
          "StorageClass": "GLACIER"
        }
      ],
      "NoncurrentVersionExpiration": {
        "NoncurrentDays": 90
      }
    },
    {
      "Id": "DeleteTempFiles",
      "Status": "Enabled",
      "Filter": {
        "Prefix": "temp/"
      },
      "Expiration": {
        "Days": 7
      }
    }
  ]
}
```

```bash
# Appliquer la politique de cycle de vie
aws s3api put-bucket-lifecycle-configuration \
  --bucket m2i-source-anselme-bucket \
  --lifecycle-configuration file://lifecycle-policy.json

echo "Politique de cycle de vie appliquee."

# VÃ©rifier
aws s3api get-bucket-lifecycle-configuration \
  --bucket m2i-source-anselme-bucket
```

---

## ğŸŒ PARTIE 6 : AccÃ¨s cross-account (Bonus)

### ScÃ©nario : Permettre Ã  un autre compte AWS de lire votre bucket

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowCrossAccountRead",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::999999999999:root"
      },
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::m2i-source-anselme-bucket",
        "arn:aws:s3:::m2i-source-anselme-bucket/*"
      ]
    }
  ]
}
```

**ğŸ’¡ Note** : Remplacez `999999999999` par l'Account ID du compte destinataire.

---

## ğŸ“Š PARTIE 7 : Monitoring et logs

### Activer les logs d'accÃ¨s S3

```bash
# CrÃ©er un bucket pour les logs
aws s3 mb s3://m2i-logs-anselme-bucket --region us-east-1

# Configurer les logs d'accÃ¨s
cat <<'EOF' > logging-config.json
{
  "LoggingEnabled": {
    "TargetBucket": "m2i-logs-anselme-bucket",
    "TargetPrefix": "source-bucket-logs/"
  }
}
EOF

aws s3api put-bucket-logging \
  --bucket m2i-source-anselme-bucket \
  --bucket-logging-status file://logging-config.json

echo "Logs d'acces S3 actives."
```

### Analyser les logs avec CloudWatch Insights

```sql
-- RequÃªte CloudWatch Logs Insights pour analyser les accÃ¨s S3
fields @timestamp, operation, key, remoteip
| filter bucket = "m2i-source-anselme-bucket"
| filter operation = "REST.GET.OBJECT"
| stats count() by key
| sort count desc
| limit 20
```

---

## ğŸ§¹ PARTIE 8 : Nettoyage

### Supprimer tous les objets et buckets

```bash
# âš ï¸ ATTENTION : Cette opÃ©ration est irrÃ©versible !

buckets=(
  "m2i-source-anselme-bucket"
  "m2i-dest1-anselme-bucket"
  "m2i-dest2-anselme-bucket"
  "m2i-logs-anselme-bucket"
)

for bucket in "${buckets[@]}"; do
  echo "Suppression de $bucket..."

  # Desactiver le versioning
  aws s3api put-bucket-versioning \
    --bucket "$bucket" \
    --versioning-configuration Status=Suspended >/dev/null 2>&1 || true

  # Supprimer tous les objets (y compris les versions)
  aws s3 rm "s3://$bucket" --recursive --quiet || true

  # Supprimer toutes les versions et delete markers
  versions=$(aws s3api list-object-versions --bucket "$bucket" \
    --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' --output json)
  if [ "$versions" != '{"Objects": []}' ]; then
    aws s3api delete-objects --bucket "$bucket" --delete "$versions" >/dev/null 2>&1 || true
  fi

  markers=$(aws s3api list-object-versions --bucket "$bucket" \
    --query '{Objects: DeleteMarkers[].{Key:Key,VersionId:VersionId}}' --output json)
  if [ "$markers" != '{"Objects": []}' ]; then
    aws s3api delete-objects --bucket "$bucket" --delete "$markers" >/dev/null 2>&1 || true
  fi

  # Supprimer le bucket
  aws s3 rb "s3://$bucket" --force || true

  echo "$bucket supprime"
done

# Supprimer le role IAM
aws iam delete-role-policy \
  --role-name S3-Replication-Role \
  --policy-name S3-Replication-Policy

aws iam delete-role --role-name S3-Replication-Role

echo "Nettoyage termine."
```

---

## ğŸ“š Ressources et sources

### Documentation AWS officielle

1. **Amazon S3 User Guide**
   - Source: https://docs.aws.amazon.com/s3/
   - Guide complet sur Amazon S3

2. **S3 Bucket Policies**
   - Source: https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html
   - Documentation sur les politiques de bucket

3. **S3 Replication**
   - Source: https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html
   - Guide de rÃ©plication (SRR et CRR)

4. **S3 Lifecycle Configuration**
   - Source: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html
   - Configuration du cycle de vie des objets

5. **Cross-Account Access**
   - Source: https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html
   - AccÃ¨s entre comptes AWS

### Workshops et tutoriels AWS

1. **AWS Workshops - Storage**
   - Source: https://workshops.aws/categories/Storage
   - Catalogue des workshops sur le stockage

2. **AWS Skill Builder - S3 Courses**
   - Source: https://explore.skillbuilder.aws/learn/course/internal/view/elearning/53/introduction-to-amazon-simple-storage-service-s3
   - Cours gratuits sur S3

3. **AWS Samples - S3 Examples**
   - Source: https://github.com/aws-samples/amazon-s3-examples
   - Exemples de code pour S3

### VidÃ©os et prÃ©sentations

- **AWS re:Invent - S3 Deep Dive Sessions**
  - Rechercher "S3 best practices" sur YouTube AWS Channel
  - Sessions techniques approfondies sur S3

### Outils recommandÃ©s

- **AWS CLI** : https://aws.amazon.com/cli/
- **S3 Browser** (GUI) : https://s3browser.com/
- **CloudBerry Explorer** : https://www.cloudberrylab.com/explorer/amazon-s3.aspx

---

## ğŸ¯ Points clÃ©s Ã  retenir

âœ… **Bucket Policies** : ContrÃ´le d'accÃ¨s au niveau du bucket  
âœ… **IAM Policies** : ContrÃ´le d'accÃ¨s au niveau de l'utilisateur/rÃ´le  
âœ… **S3 Replication** : Copie automatique (SRR et CRR)  
âœ… **Lifecycle Policies** : Gestion automatique du cycle de vie  
âœ… **Versioning** : Obligatoire pour la rÃ©plication  
âœ… **Cross-Account** : Partage sÃ©curisÃ© entre comptes AWS  

---

## ğŸ’° Estimation des coÃ»ts

| Ressource | QuantitÃ© | CoÃ»t estimÃ© |
|-----------|----------|-------------|
| Stockage S3 (STANDARD) | 1 GB | ~$0.023/mois |
| RequÃªtes PUT/COPY | 1000 | ~$0.005 |
| Transfert cross-region | 1 GB | ~$0.02 |
| RÃ©plication (RTC) | Si activÃ©e | ~$0.015/GB |
| **Total (lab complet)** | | **~$0.10** |

ğŸ’¡ **Note** : CoÃ»ts trÃ¨s faibles pour un lab, pensez Ã  nettoyer aprÃ¨s !

---

**ğŸ‰ FÃ©licitations !** Vous maÃ®trisez maintenant les politiques S3 et les transferts de fichiers entre buckets ! ğŸš€
